{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords    \n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tag import pos_tag\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "\n",
    "import textstat\n",
    "from lexicalrichness import LexicalRichness\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.graph_objects as go\n",
    "# Fake:1, Real: 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix  \n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_trainset(csv_file):\n",
    "    df_raw = pd.read_csv(csv_file, sep='\\n')\n",
    "    df_raw.rename(columns = {'text\\tlabel':'text'}, inplace = True)\n",
    "    df_raw = pd.DataFrame(df_raw.text.str.rsplit('\\t',1).tolist(),\n",
    "                          columns = ['text', 'label'])\n",
    "    return df_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_trainset('train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing text for Vectorizor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1) Lowercase the text: This preprocessing step is done so words can later be cross checked with the stopwords and pos_tag dictionaries.\n",
    "2) Remove words with just one letter\n",
    "3) Remove words that contain numbers\n",
    "4) Tokenize the text and remove punctuation\n",
    "5) Remove stopwords: Proper analysis of text usually relies on the most recurring words. Stopwords including words as “the”, “as” and “and” appear a lot in a text, but eech word doesn’t really give relevant explanation, so one of NLP comon practices is to remove such words.\n",
    "6) Remove empty tokens: After tokenization, we have to make sure all tokens taken into account contribute to the label prediction.\n",
    "7) Lemmatize the text: In order to normalize the text, we apply lemmatization. In this way, words with the same root are processed equally e.g. when the words “took” or “taken” are found in the text, they are lemmatized to “take”, infinitive of the verb.\n",
    "\"\"\"\n",
    "\n",
    "def get_wordnet_pos(pos_tag):\n",
    "    if pos_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif pos_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif pos_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif pos_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "    \n",
    "def remove_URL(t):\n",
    "    return re.sub(r\"http\\S+\",\"\", t)\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def preprocess(text):\n",
    "\n",
    "    text = remove_URL(text)\n",
    "    text = text.lower().replace(\"-\",\" \").replace(\".\",\" \").replace(\":\",\" \")\n",
    "    custom_s = string.punctuation.replace(\"-\",\"”\")+\"‘\"+\"“\"+\".\"+\",\"\n",
    "    for i in custom_s:\n",
    "        text = text.replace(i,\"\")\n",
    "    text = [t for t in text.split(\" \") if len(t) > 1] # a list of words\n",
    "    text = [word for word in text if not any(c.isdigit() for c in word)]    \n",
    "    text = [x.strip() for x in text if x not in stop_words]\n",
    "    text = [t for t in text if len(t) > 0] # remove empty token\n",
    "    text = [ps.stem(x) for x in text]\n",
    "#     pos_tags = pos_tag(text)\n",
    "#     text = [WordNetLemmatizer().lemmatize(t[0], get_wordnet_pos(t[1])) for t in pos_tags]\n",
    "    text = \" \".join(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.text = df.text.apply(preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Pipeline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(df.text, \n",
    "                                                 df.label, \n",
    "                                                 test_size=0.2, \n",
    "                                                 random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_vectorizer = CountVectorizer(analyzer = \"word\", ngram_range=(1,3))\n",
    "\n",
    "classifier = LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
    "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
    "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
    "          tol=0.0001, verbose=0, warm_start=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/makhy/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning:\n",
      "\n",
      "Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "\n",
      "/Users/makhy/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning:\n",
      "\n",
      "Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 81.36%\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([('vect', CountVectorizer(analyzer = \"word\", ngram_range=(1,3))),\n",
    "                 ('tfidf', TfidfTransformer(norm=\"l2\")),\n",
    "                 ('model', classifier)\n",
    "                  ])\n",
    "# Fitting the model\n",
    "model = pipe.fit(X_train, y_train)\n",
    "# Accuracy\n",
    "prediction = model.predict(X_test)\n",
    "print(\"accuracy: {}%\".format(round(accuracy_score(y_test, prediction)*100,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_testset(csv_file):\n",
    "    df_raw = pd.read_csv(csv_file, sep='\\n')\n",
    "    df_raw.rename(columns = {'id\\ttext':'text'}, inplace = True)\n",
    "    df_raw = pd.DataFrame(df_raw.text.str.split('\\t',1).tolist(),\n",
    "                          columns = ['id', 'text'])\n",
    "    return df_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = load_testset('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.text = test_df.text.apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'best song year best track matter rank fader stream appl music spotifi'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.text[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = model.predict(test_df.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['label'] = test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        817\n",
       "1        429\n",
       "label      1\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_test = test_df[['id','label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_test.to_csv(\"sub_0811B.csv\",index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
